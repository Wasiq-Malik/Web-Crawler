{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler\n",
    "\n",
    "This notebook contains started code structure for creating a crawler on single machine\n",
    "\n",
    "**Author:** Noshaba Nasir\n",
    "\n",
    "**Date:** 26/3/2021\n",
    "\n",
    "**Updated by:** Muhammad Wasiq 17L-6315\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any library to be imported here\n",
    "import os \n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "import threading\n",
    "from time import time\n",
    "from time import sleep\n",
    "from numpy.random import choice\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue\n",
    "from queue import PriorityQueue\n",
    "\n",
    "import urllib.robotparser as robo\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import  urljoin\n",
    "from urllib.parse import urldefrag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawler Parameters\n",
    "BACKQUEUES = 3\n",
    "THREADS = 3*BACKQUEUES\n",
    "FRONTQUEUES = 5\n",
    "WAITTIME = 15 ; # wait 15 seconds before fetching URLS from \n",
    "\n",
    "# Add any other global parameters here\n",
    "SCRAPED_DATA = []\n",
    "CRAWLERD_ROBOTS = {}\n",
    "CRAWLED_URLS = set()\n",
    "CRAWL_LIMIT = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRONTIER\n",
    "\n",
    "Frontier should use the Mercator frontier design as discussed in lecture.\n",
    "\n",
    "Preferably it should be a class and should have the given functions.\n",
    "\n",
    "*prioritizer* function is a stub right now, it will return a random number  between 1 to f for given URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class frontier:\n",
    "# add the code for frontier here\n",
    "# should have functions __init__, get_URL, add_URLs, add_to_backqueue\n",
    "    def __init__(self, seeds, front_count, back_count):\n",
    "\n",
    "        # initializing required data structures\n",
    "        self.front_queues = [Queue() for _ in range(front_count)]\n",
    "        self.back_queues = [Queue() for _ in range(back_count)]\n",
    "        self.domain_map = {}\n",
    "        self.back_selector = PriorityQueue()\n",
    "        \n",
    "        self.add_URLs(seeds)   # insert seeds in front queues\n",
    "\n",
    "        # check if minimum number of seeds available\n",
    "        unique_seeds = set()\n",
    "        for url in seeds:\n",
    "            domain = urlparse(url).netloc\n",
    "            unique_seeds.add(domain)\n",
    "\n",
    "        # throw error if not enough seeds\n",
    "        if len(unique_seeds) < back_count:\n",
    "            raise Exception('Not enough unique seed domains')\n",
    "        \n",
    "        # fill each back queue with atleast 1 seed\n",
    "        for i in range(back_count):\n",
    "            self.add_to_backqueue(i)            \n",
    "            self.back_selector.put((time() - WAITTIME, i)) # intialize the heap as well\n",
    "        \n",
    "    # add more functions here\n",
    "    def fetch_from_front(self):\n",
    "        \n",
    "        k = len(self.front_queues)\n",
    "        total = (k * (k + 1)) // 2 # arithmatic sum\n",
    "\n",
    "        # choose a queue to extract url from based on priority-weighted probability\n",
    "        idx = choice([i for i in range(k)], 1, p=[(i+1)/total for i in range(k)])[0]\n",
    "        while self.front_queues[idx].empty():\n",
    "            idx = choice([i for i in range(k)], 1, p=[(i+1)/total for i in range(k)])[0]\n",
    "\n",
    "        return self.front_queues[idx].get()\n",
    "\n",
    "    def get_URL(self):\n",
    "\n",
    "        timestamp, idx = self.back_selector.get()\n",
    "        time_elapsed = time() - timestamp\n",
    "\n",
    "        # calculate thread waiting time to maintain politeness\n",
    "        sleep_time = 0\n",
    "        if time_elapsed < WAITTIME:\n",
    "            sleep_time = int(WAITTIME - time_elapsed)\n",
    "        \n",
    "        next_url = self.back_queues[idx].get()  # get new url\n",
    "          \n",
    "        if self.back_queues[idx].empty():\n",
    "            # remove the domain assigned to i'th queue\n",
    "            self.domain_map = { k : v for k,v in self.domain_map.items() if v != idx}\n",
    "            \n",
    "            # fetch new domain for back queue\n",
    "            self.add_to_backqueue(idx)\n",
    "\n",
    "        self.back_selector.put((time() + sleep_time, idx)) # update the timestamp\n",
    "\n",
    "        return next_url, sleep_time\n",
    "            \n",
    "\n",
    "    def add_URLs(self, URLs):\n",
    "        # insert all provided URLs into front queues, assuming they're new\n",
    "        for url in URLs:\n",
    "            priority = prioritizer(url, len(self.front_queues)) # assign priority to the URL\n",
    "            self.front_queues[priority - 1].put(url)    # index using priority value to insert URl\n",
    "\n",
    "\n",
    "    def add_to_backqueue(self, idx):\n",
    "        \n",
    "        while self.back_queues[idx].empty():\n",
    "            # bring a url from a front queue to be added to a back queue\n",
    "            url = self.fetch_from_front()\n",
    "            url_domain = urlparse(url).netloc\n",
    "\n",
    "            # insert fetched url in coresponding back queue\n",
    "            if url_domain in self.domain_map:\n",
    "                self.back_queues[self.domain_map[url_domain]].put(url)\n",
    "            else:\n",
    "                self.domain_map[url_domain] = idx\n",
    "                self.back_queues[idx].put(url)\n",
    "\n",
    "\n",
    "def prioritizer(URL,f):\n",
    "    \"\"\"\n",
    "    Take URL and returns priority from 1 to F\n",
    "    Right now it like a stub function. \n",
    "    It will return a random number from 1 to f for given inputs. \n",
    "    \"\"\"\n",
    "    return random.randint(1,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL Fetching and Filtering Pipeline\n",
    "\n",
    "Get HTML by requesting given URL.\n",
    "Parse HTML and extract all URLs.\n",
    "Filter the URLS that are in robots.txt files of server and the have been already processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(URL):\n",
    "    # hit given URL and return response\n",
    "    try:\n",
    "        response = requests.get(URL)\n",
    "\n",
    "        if 'Content-Type' in response.headers and 'html' not in response.headers['Content-Type']:\n",
    "            raise Exception(f\"Did not get a HTML response from {URl}\")\n",
    "\n",
    "        # save the html response as text file\n",
    "        # file_name = '_'.join(URL.split('/')[2:])\n",
    "        # with open(f'{file_name}.txt', 'w+', encoding=\"utf-8\") as file:\n",
    "        #      file.write(response.text)\n",
    "            \n",
    "        return response.content\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(\"Get Request Failed. \" + str(e))\n",
    "\n",
    "\n",
    "def parse_data(URL, html_doc):\n",
    "    parsed_url = urlparse(URL)  \n",
    "    domain = 'https://' + parsed_url.netloc\n",
    "    \n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')   # use bs4 html parser\n",
    "\n",
    "    # convert relative paths to absolutes if found\n",
    "    links = [urljoin(domain, link.get('href')) for link in soup.find_all('a')] \n",
    "\n",
    "    # remove fragment identifiers from the links\n",
    "    links = [urldefrag(link)[0] for link in links]  \n",
    "\n",
    "    # remove duplicate links due to defragmentation\n",
    "    links = list(set(links))    \n",
    "\n",
    "    return links\n",
    "\n",
    "def filter_URLs(URL, fetched_URLs):\n",
    "  \n",
    "    # politeness; only keep URLs that the domain allows\n",
    "    filtered_urls = set()\n",
    "    for url in fetched_URLs:\n",
    "        # generating robots.txt URL\n",
    "        hostname = urlparse(URL).netloc \n",
    "\n",
    "        robot_parser = None\n",
    "        # if host already visited, lookup robot.txt\n",
    "        if hostname in CRAWLERD_ROBOTS:\n",
    "            robot_parser = CRAWLERD_ROBOTS[hostname]\n",
    "        else:\n",
    "            # if not, fetch robot.txt\n",
    "            robot_link = 'http://' + hostname + '/robots.txt'\n",
    "            robot_parser = robo.RobotFileParser(robot_link)\n",
    "            robot_parser.read()\n",
    "\n",
    "            CRAWLERD_ROBOTS[hostname] = robot_parser\n",
    "            \n",
    "        if robot_parser.can_fetch('*', url):\n",
    "            filtered_urls.add(url)\n",
    "\n",
    "    return list(filtered_urls)\n",
    "\n",
    "def dup_elimination(url_set, filtered_URLs):\n",
    "    unique_urls = []\n",
    "    for url in filtered_URLs:\n",
    "        if url in url_set:\n",
    "            continue\n",
    "        unique_urls.append(url)\n",
    "\n",
    "    return unique_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theard task\n",
    "mutex = threading.Lock()\n",
    "def crawler_thread_task(frontier):\n",
    "\n",
    "    while True:\n",
    "\n",
    "        mutex.acquire()\n",
    "        # critical section - start\n",
    "        if len(CRAWLED_URLS) >= CRAWL_LIMIT:\n",
    "            mutex.release()\n",
    "            break\n",
    "        \n",
    "        curr_url, waiting_time = frontier.get_URL()\n",
    "        CRAWLED_URLS.add(curr_url)\n",
    "\n",
    "        print(f\"[{threading.current_thread().name}] Got URL [{len(CRAWLED_URLS)}/{CRAWL_LIMIT}] \")\n",
    "        \n",
    "        # critical section - end\n",
    "        mutex.release()\n",
    "\n",
    "        try:\n",
    "            sleep(waiting_time)\n",
    "            curr_domain = urlparse(curr_url).netloc\n",
    "            print(f\"[{threading.current_thread().name}] Crawling {curr_domain} ... \") \n",
    "\n",
    "            data = fetch_data(curr_url)\n",
    "            print(f\"[{threading.current_thread().name}] Fetched {curr_url}\")\n",
    "\n",
    "            new_urls = parse_data(curr_url, data)\n",
    "            print(f\"[{threading.current_thread().name}] Saved {curr_url}\")\n",
    "\n",
    "            filtered_urls = filter_URLs(curr_url, new_urls)\n",
    "            print(f\"[{threading.current_thread().name}] Generating new URLs ... \")\n",
    "\n",
    "            final_urls = dup_elimination(url_set=CRAWLED_URLS, filtered_URLs=filtered_urls)\n",
    "            print(f\"[{threading.current_thread().name}] Generated {len(final_urls)} new URLs.\")\n",
    "\n",
    "            frontier.add_URLs(final_urls)\n",
    "            print(f\"[{threading.current_thread().name}] Crawled {curr_domain}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{threading.current_thread().name}] Failed to Crawl {curr_url} ... \")\n",
    "            print(f\"[{threading.current_thread().name} Exception]\", e)\n",
    "            continue\n",
    "    \n",
    "    print(f\"[{threading.current_thread().name}] Exiting ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# intialize every thing\n",
    "seed_urls = [\n",
    "    'https://docs.oracle.com/en/', \n",
    "    'https://www.oracle.com/corporate/',\n",
    "    'https://en.wikipedia.org/wiki/Machine_learning',\n",
    "    'https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html',\n",
    "    'https://docs.oracle.com/middleware/jet210/jet/index.html',\n",
    "    'https://en.wikipedia.org/w/api.php',\n",
    "    'https://en.wikipedia.org/api/',\n",
    "    'https://en.wikipedia.org/wiki/Weka_(machine_learning)'\n",
    "    ]\n",
    "\n",
    "URL_frontier = frontier(seeds=seed_urls, front_count=FRONTQUEUES, back_count=BACKQUEUES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " URL [55/100] \n",
      "[t3] Crawling zh.wikipedia.org ... \n",
      "[t3] Fetched https://zh.wikipedia.org/wiki/Weka\n",
      "[t3] Saved https://zh.wikipedia.org/wiki/Weka\n",
      "[t3] Generating new URLs ... \n",
      "[t3] Generated 70 new URLs.\n",
      "[t3] Crawled zh.wikipedia.org\n",
      "[t3] Got URL [56/100] \n",
      "[t4] Crawling en.wikipedia.org ... \n",
      "[t4] Fetched https://en.wikipedia.org/wiki/Cluster_analysis\n",
      "[t4] Saved https://en.wikipedia.org/wiki/Cluster_analysis\n",
      "[t4] Generating new URLs ... \n",
      "[t4] Generated 775 new URLs.\n",
      "[t4] Crawled en.wikipedia.org\n",
      "[t4] Got URL [57/100] \n",
      "[t5] Crawling www.wikimediafoundation.org ... \n",
      "[t5] Fetched https://www.wikimediafoundation.org/\n",
      "[t5] Saved https://www.wikimediafoundation.org/\n",
      "[t6] Crawling stats.wikimedia.org ... \n",
      "[t6] Fetched https://stats.wikimedia.org/\n",
      "[t6] Saved https://stats.wikimedia.org/\n",
      "[t5] Generating new URLs ... \n",
      "[t5] Generated 77 new URLs.\n",
      "[t5] Crawled www.wikimediafoundation.org\n",
      "[t5] Got URL [58/100] \n",
      "[t6] Generating new URLs ... \n",
      "[t6] Generated 0 new URLs.\n",
      "[t6] Crawled stats.wikimedia.org\n",
      "[t6] Got URL [59/100] \n",
      "[t7] Crawling en.wikipedia.org ... \n",
      "[t7] Fetched https://en.wikipedia.org/wiki/Makefile\n",
      "[t7] Saved https://en.wikipedia.org/wiki/Makefile\n",
      "[t7] Generating new URLs ... \n",
      "[t7] Generated 344 new URLs.\n",
      "[t7] Crawled en.wikipedia.org\n",
      "[t7] Got URL [60/100] \n",
      "[t9] Crawling ru.wikipedia.org ... \n",
      "[t8] Crawling community.hitachivantara.com ... \n",
      "[t9] Fetched https://ru.wikipedia.org/wiki/Weka\n",
      "[t9] Saved https://ru.wikipedia.org/wiki/Weka\n",
      "[t8] Failed to Crawl https://community.hitachivantara.com/s/article/machine-intelligence-made-easy ... \n",
      "[t8 Exception] Get Request Failed. HTTPSConnectionPool(host='community.hitachivantara.com', port=443): Max retries exceeded with url: /s/article/machine-intelligence-made-easy (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1123)')))\n",
      "[t8] Got URL [61/100] \n",
      "[t9] Generating new URLs ... \n",
      "[t9] Generated 95 new URLs.\n",
      "[t9] Crawled ru.wikipedia.org\n",
      "[t9] Got URL [62/100] \n",
      "[t2] Crawling en.wikipedia.org ... \n",
      "[t2] Fetched https://en.wikipedia.org/wiki/Machine_learning\n",
      "[t2] Saved https://en.wikipedia.org/wiki/Machine_learning\n",
      "[t2] Generating new URLs ... \n",
      "[t2] Generated 918 new URLs.\n",
      "[t2] Crawled en.wikipedia.org\n",
      "[t2] Got URL [63/100] \n",
      "[t1] Crawling creativecommons.org ... \n",
      "[t3] Crawling www.oracle.com ... \n",
      "[t1] Fetched https://creativecommons.org/licenses/by-sa/3.0/\n",
      "[t1] Saved https://creativecommons.org/licenses/by-sa/3.0/\n",
      "[t1] Generating new URLs ... \n",
      "[t1] Generated 0 new URLs.\n",
      "[t1] Crawled creativecommons.org\n",
      "[t1] Got URL [64/100] \n",
      "[t3] Fetched https://www.oracle.com/corporate/careers/culture/diversity.html\n",
      "[t3] Saved https://www.oracle.com/corporate/careers/culture/diversity.html\n",
      "[t3] Generating new URLs ... \n",
      "[t3] Generated 98 new URLs.\n",
      "[t3] Crawled www.oracle.com\n",
      "[t3] Got URL [65/100] \n",
      "[t4] Crawling en.wikipedia.org ... \n",
      "[t4] Fetched https://en.wikipedia.org/wiki/File:Weka-3.5.5.png\n",
      "[t4] Saved https://en.wikipedia.org/wiki/File:Weka-3.5.5.png\n",
      "[t4] Generating new URLs ... \n",
      "[t4] Generated 31 new URLs.\n",
      "[t4] Crawled en.wikipedia.org\n",
      "[t4] Got URL [66/100] \n",
      "[t5] Crawling sl.wikipedia.org ... \n",
      "[t5] Fetched https://sl.wikipedia.org/wiki/Weka_(strojno_u%C4%8Denje)\n",
      "[t5] Saved https://sl.wikipedia.org/wiki/Weka_(strojno_u%C4%8Denje)\n",
      "[t5] Generating new URLs ... \n",
      "[t5] Generated 34 new URLs.\n",
      "[t5] Crawled sl.wikipedia.org\n",
      "[t5] Got URL [67/100] \n",
      "[t6] Crawling www.oracle.com ... \n",
      "[t6] Fetched https://www.oracle.com/legal/privacy/\n",
      "[t6] Saved https://www.oracle.com/legal/privacy/\n",
      "[t6] Generating new URLs ... \n",
      "[t6] Generated 43 new URLs.\n",
      "[t6] Crawled www.oracle.com\n",
      "[t6] Got URL [68/100] \n",
      "[t7] Crawling en.wikipedia.org ... \n",
      "[t7] Fetched https://en.wikipedia.org/wiki/Salome_(software)\n",
      "[t7] Saved https://en.wikipedia.org/wiki/Salome_(software)\n",
      "[t7] Generating new URLs ... \n",
      "[t7] Generated 279 new URLs.\n",
      "[t7] Crawled en.wikipedia.org\n",
      "[t7] Got URL [69/100] \n",
      "[t8] Crawling nl.wikipedia.org ... \n",
      "[t8] Fetched https://nl.wikipedia.org/wiki/Weka_(software)\n",
      "[t8] Saved https://nl.wikipedia.org/wiki/Weka_(software)\n",
      "[t8] Generating new URLs ... \n",
      "[t8] Generated 59 new URLs.\n",
      "[t8] Crawled nl.wikipedia.org\n",
      "[t8] Got URL [70/100] \n",
      "[t9] Crawling www.oracle.com ... \n",
      "[t9] Fetched https://www.oracle.com/internet-of-things/\n",
      "[t9] Saved https://www.oracle.com/internet-of-things/\n",
      "[t9] Generating new URLs ... \n",
      "[t9] Generated 71 new URLs.\n",
      "[t9] Crawled www.oracle.com\n",
      "[t9] Got URL [71/100] \n",
      "[t2] Crawling en.wikipedia.org ... \n",
      "[t2] Fetched https://en.wikipedia.org/wiki/Kevin_Leyton-Brown\n",
      "[t2] Saved https://en.wikipedia.org/wiki/Kevin_Leyton-Brown\n",
      "[t2] Generating new URLs ... \n",
      "[t2] Generated 101 new URLs.\n",
      "[t2] Crawled en.wikipedia.org\n",
      "[t2] Got URL [72/100] \n",
      "[t1] Crawling go.oracle.com ... \n",
      "[t1] Fetched https://go.oracle.com/subscriptions\n",
      "[t1] Saved https://go.oracle.com/subscriptions\n",
      "[t3] Crawling www.oracle.com ... \n",
      "[t1] Generating new URLs ... \n",
      "[t1] Generated 0 new URLs.\n",
      "[t1] Crawled go.oracle.com\n",
      "[t1] Got URL [73/100] \n",
      "[t3] Fetched https://www.oracle.com/blockchain/\n",
      "[t3] Saved https://www.oracle.com/blockchain/\n",
      "[t3] Generating new URLs ... \n",
      "[t3] Generated 70 new URLs.\n",
      "[t3] Crawled www.oracle.com\n",
      "[t3] Got URL [74/100] \n",
      "[t4] Crawling en.wikipedia.org ... \n",
      "[t4] Fetched https://en.wikipedia.org/wiki/Fortress_(programming_language)\n",
      "[t4] Saved https://en.wikipedia.org/wiki/Fortress_(programming_language)\n",
      "[t4] Generating new URLs ... \n",
      "[t4] Generated 203 new URLs.\n",
      "[t4] Crawled en.wikipedia.org\n",
      "[t4] Got URL [75/100] \n",
      "[t5] Crawling ja.wikipedia.org ... \n",
      "[t5] Fetched https://ja.wikipedia.org/wiki/Weka\n",
      "[t5] Saved https://ja.wikipedia.org/wiki/Weka\n",
      "[t5] Generating new URLs ... \n",
      "[t5] Generated 105 new URLs.\n",
      "[t5] Crawled ja.wikipedia.org\n",
      "[t5] Got URL [76/100] \n",
      "[t6] Crawling www.oracle.com ... \n",
      "[t7] Crawling en.wikipedia.org ... \n",
      "[t7] Fetched https://en.wikipedia.org/wiki/Wolfram_Mathematica\n",
      "[t7] Saved https://en.wikipedia.org/wiki/Wolfram_Mathematica\n",
      "[t7] Generating new URLs ... \n",
      "[t7] Generated 557 new URLs.\n",
      "[t7] Crawled en.wikipedia.org\n",
      "[t7] Got URL [77/100] \n",
      "[t8] Crawling es.wikipedia.org ... \n",
      "[t8] Fetched https://es.wikipedia.org/wiki/Weka_(aprendizaje_autom%C3%A1tico)\n",
      "[t8] Saved https://es.wikipedia.org/wiki/Weka_(aprendizaje_autom%C3%A1tico)\n",
      "[t8] Generating new URLs ... \n",
      "[t8] Generated 125 new URLs.\n",
      "[t8] Crawled es.wikipedia.org\n",
      "[t8] Got URL [78/100] \n",
      "[t9] Crawling www.oracle.com ... \n",
      "[t9] Fetched https://www.oracle.com/startup/\n",
      "[t9] Saved https://www.oracle.com/startup/\n",
      "[t9] Generating new URLs ... \n",
      "[t9] Generated 56 new URLs.\n",
      "[t9] Crawled www.oracle.com\n",
      "[t9] Got URL [79/100] \n",
      "[t2] Crawling en.wikipedia.org ... \n",
      "[t1] Crawling citeseerx.ist.psu.edu ... \n",
      "[t2] Fetched https://en.wikipedia.org/wiki/Wikipedia:Community_portal\n",
      "[t2] Saved https://en.wikipedia.org/wiki/Wikipedia:Community_portal\n",
      "[t2] Generating new URLs ... \n",
      "[t2] Generated 2035 new URLs.\n",
      "[t2] Crawled en.wikipedia.org\n",
      "[t2] Got URL [80/100] \n",
      "[t3] Crawling www.oracle.com ... \n",
      "[t1] Fetched https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.459.8443\n",
      "[t1] Saved https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.459.8443\n",
      "[t1] Generating new URLs ... \n",
      "[t1] Generated 46 new URLs.\n",
      "[t1] Crawled citeseerx.ist.psu.edu\n",
      "[t1] Got URL [81/100] \n",
      "[t3] Fetched https://www.oracle.com/tel:18006330738\n",
      "[t3] Saved https://www.oracle.com/tel:18006330738\n",
      "[t3] Generating new URLs ... \n",
      "[t3] Generated 0 new URLs.\n",
      "[t3] Crawled www.oracle.com\n",
      "[t3] Got URL [82/100] \n",
      "[t4] Crawling en.wikipedia.org ... \n",
      "[t4] Fetched https://en.wikipedia.org/wiki/Massive_Online_Analysis\n",
      "[t4] Saved https://en.wikipedia.org/wiki/Massive_Online_Analysis\n",
      "[t4] Generating new URLs ... \n",
      "[t4] Generated 98 new URLs.\n",
      "[t4] Crawled en.wikipedia.org\n",
      "[t4] Got URL [83/100] \n",
      "[t5] Crawling dl.acm.org ... \n",
      "[t6] Fetched https://www.oracle.com/corporate/careers/benefits/training.html\n",
      "[t6] Saved https://www.oracle.com/corporate/careers/benefits/training.html\n",
      "[t6] Generating new URLs ... \n",
      "[t6] Generated 52 new URLs.\n",
      "[t6] Crawled www.oracle.com\n",
      "[t6] Got URL [84/100] \n",
      "[t7] Crawling www.oracle.com ... \n",
      "[t7] Fetched https://www.oracle.com/universal-menu/\n",
      "[t7] Saved https://www.oracle.com/universal-menu/\n",
      "[t7] Generating new URLs ... \n",
      "[t7] Generated 201 new URLs.\n",
      "[t7] Crawled www.oracle.com\n",
      "[t7] Got URL [85/100] \n",
      "[t5] Fetched https://dl.acm.org/citation.cfm?id=2487629\n",
      "[t5] Saved https://dl.acm.org/citation.cfm?id=2487629\n",
      "[t5] Generating new URLs ... \n",
      "[t5] Generated 0 new URLs.\n",
      "[t5] Crawled dl.acm.org\n",
      "[t5] Got URL [86/100] \n",
      "[t8] Crawling en.wikipedia.org ... \n",
      "[t8] Fetched https://en.wikipedia.org/wiki/Maple_(software)\n",
      "[t8] Saved https://en.wikipedia.org/wiki/Maple_(software)\n",
      "[t8] Generating new URLs ... \n",
      "[t8] Generated 460 new URLs.\n",
      "[t8] Crawled en.wikipedia.org\n",
      "[t8] Got URL [87/100] \n",
      "[t9] Crawling eu.wikipedia.org ... \n",
      "[t9] Fetched https://eu.wikipedia.org/wiki/Weka_(ikasketa_automatikoa)\n",
      "[t9] Saved https://eu.wikipedia.org/wiki/Weka_(ikasketa_automatikoa)\n",
      "[t9] Generating new URLs ... \n",
      "[t9] Generated 69 new URLs.\n",
      "[t9] Crawled eu.wikipedia.org\n",
      "[t9] Got URL [88/100] \n",
      "[t2] Crawling www.oracle.com ... \n",
      "[t2] Fetched https://www.oracle.com/corporate/careers/culture/who-we-are.html\n",
      "[t2] Saved https://www.oracle.com/corporate/careers/culture/who-we-are.html\n",
      "[t2] Generating new URLs ... \n",
      "[t2] Generated 55 new URLs.\n",
      "[t2] Crawled www.oracle.com\n",
      "[t2] Got URL [89/100] \n",
      "[t1] Crawling en.wikipedia.org ... \n",
      "[t1] Fetched https://en.wikipedia.org/wiki/Endemism\n",
      "[t1] Saved https://en.wikipedia.org/wiki/Endemism\n",
      "[t1] Generating new URLs ... \n",
      "[t1] Generated 251 new URLs.\n",
      "[t1] Crawled en.wikipedia.org\n",
      "[t1] Got URL [90/100] \n",
      "[t3] Crawling vi.wikipedia.org ... \n",
      "[t3] Fetched https://vi.wikipedia.org/wiki/Weka_(h%E1%BB%8Dc_m%C3%A1y)\n",
      "[t3] Saved https://vi.wikipedia.org/wiki/Weka_(h%E1%BB%8Dc_m%C3%A1y)\n",
      "[t3] Generating new URLs ... \n",
      "[t3] Generated 69 new URLs.\n",
      "[t3] Crawled vi.wikipedia.org\n",
      "[t3] Got URL [91/100] \n",
      "[t4] Crawling www.oracle.com ... \n",
      "[t4] Fetched https://www.oracle.com/partnernetwork/\n",
      "[t4] Saved https://www.oracle.com/partnernetwork/\n",
      "[t4] Generating new URLs ... \n",
      "[t4] Generated 40 new URLs.\n",
      "[t4] Crawled www.oracle.com\n",
      "[t4] Got URL [92/100] \n",
      "[t6] Crawling en.wikipedia.org ... \n",
      "[t6] Fetched https://en.wikipedia.org/wiki/Scikit-learn\n",
      "[t6] Saved https://en.wikipedia.org/wiki/Scikit-learn\n",
      "[t6] Generating new URLs ... \n",
      "[t6] Generated 98 new URLs.\n",
      "[t6] Crawled en.wikipedia.org\n",
      "[t6] Got URL [93/100] \n",
      "[t7] Crawling docs.oracle.com ... \n",
      "[t5] Crawling www.oracle.com ... \n",
      "[t5] Fetched https://www.oracle.com/corporate/careers/\n",
      "[t5] Saved https://www.oracle.com/corporate/careers/\n",
      "[t5] Generating new URLs ... \n",
      "[t5] Generated 50 new URLs.\n",
      "[t5] Crawled www.oracle.com\n",
      "[t5] Got URL [94/100] \n",
      "[t7] Fetched https://docs.oracle.com/developer/GUID-ABE2373F-287F-4C3A-BEBD-02F179F399FD.htm\n",
      "[t7] Saved https://docs.oracle.com/developer/GUID-ABE2373F-287F-4C3A-BEBD-02F179F399FD.htm\n",
      "[t7] Generating new URLs ... \n",
      "[t7] Generated 30 new URLs.\n",
      "[t7] Crawled docs.oracle.com\n",
      "[t7] Got URL [95/100] \n",
      "[t8] Crawling en.wikipedia.org ... \n",
      "[t8] Fetched https://en.wikipedia.org/wiki/Software_release_life_cycle\n",
      "[t8] Saved https://en.wikipedia.org/wiki/Software_release_life_cycle\n",
      "[t8] Generating new URLs ... \n",
      "[t8] Generated 239 new URLs.\n",
      "[t8] Crawled en.wikipedia.org\n",
      "[t8] Got URL [96/100] \n",
      "[t9] Crawling phabricator.wikimedia.org ... \n",
      "[t2] Crawling www.oracle.com ... \n",
      "[t2] Fetched https://www.oracle.com/artificial-intelligence/\n",
      "[t2] Saved https://www.oracle.com/artificial-intelligence/\n",
      "[t2] Generating new URLs ... \n",
      "[t2] Generated 74 new URLs.\n",
      "[t2] Crawled www.oracle.com\n",
      "[t2] Got URL [97/100] \n",
      "[t9] Fetched https://phabricator.wikimedia.org/maniphest/query/GebfyV4uCaLd/\n",
      "[t9] Saved https://phabricator.wikimedia.org/maniphest/query/GebfyV4uCaLd/\n",
      "[t9] Generating new URLs ... \n",
      "[t9] Generated 999 new URLs.\n",
      "[t9] Crawled phabricator.wikimedia.org\n",
      "[t9] Got URL [98/100] \n",
      "[t1] Crawling en.wikipedia.org ... \n",
      "[t1] Fetched https://en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "[t1] Saved https://en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "[t1] Generating new URLs ... \n",
      "[t1] Generated 106 new URLs.\n",
      "[t1] Crawled en.wikipedia.org\n",
      "[t1] Got URL [99/100] \n",
      "[t3] Crawling lists.wikimedia.org ... \n",
      "[t3] Fetched https://lists.wikimedia.org/pipermail/mediawiki-api-announce/\n",
      "[t3] Saved https://lists.wikimedia.org/pipermail/mediawiki-api-announce/\n",
      "[t4] Crawling www.oracle.com ... \n",
      "[t3] Generating new URLs ... \n",
      "[t3] Generated 396 new URLs.\n",
      "[t3] Crawled lists.wikimedia.org\n",
      "[t3] Got URL [100/100] \n",
      "[t4] Fetched https://www.oracle.com/javadownload\n",
      "[t4] Saved https://www.oracle.com/javadownload\n",
      "[t4] Generating new URLs ... \n",
      "[t4] Generated 90 new URLs.\n",
      "[t4] Crawled www.oracle.com\n",
      "[t4] exiting ...\n",
      "[t6] Crawling en.wikipedia.org ... \n",
      "[t6] Fetched https://en.wikipedia.org/wiki/ScicosLab\n",
      "[t6] Saved https://en.wikipedia.org/wiki/ScicosLab\n",
      "[t6] Generating new URLs ... \n",
      "[t6] Generated 69 new URLs.\n",
      "[t6] Crawled en.wikipedia.org\n",
      "[t6] exiting ...\n",
      "[t5] Crawling www.csie.ntu.edu.tw ... \n",
      "[t5] Fetched https://www.csie.ntu.edu.tw/R_example.html\n",
      "[t5] Saved https://www.csie.ntu.edu.tw/R_example.html\n",
      "[t5] Generating new URLs ... \n",
      "[t5] Generated 0 new URLs.\n",
      "[t5] Crawled www.csie.ntu.edu.tw\n",
      "[t5] exiting ...\n",
      "[t7] Crawling www.oracle.com ... \n",
      "[t7] Fetched https://www.oracle.com/\n",
      "[t7] Saved https://www.oracle.com/\n",
      "[t7] Generating new URLs ... \n",
      "[t7] Generated 2 new URLs.\n",
      "[t7] Crawled www.oracle.com\n",
      "[t7] exiting ...\n",
      "[t8] Crawling en.wikipedia.org ... \n",
      "[t2] Crawling developer.oracle.com ... \n",
      "[t8] Fetched https://en.wikipedia.org/wiki/Category:Free_science_software\n",
      "[t8] Saved https://en.wikipedia.org/wiki/Category:Free_science_software\n",
      "[t8] Generating new URLs ... \n",
      "[t8] Generated 198 new URLs.\n",
      "[t8] Crawled en.wikipedia.org\n",
      "[t8] exiting ...\n",
      "[t2] Fetched https://developer.oracle.com/\n",
      "[t2] Saved https://developer.oracle.com/\n",
      "[t9] Crawling www.oracle.com ... \n",
      "[t2] Generating new URLs ... \n",
      "[t2] Generated 82 new URLs.\n",
      "[t2] Crawled developer.oracle.com\n",
      "[t2] exiting ...\n",
      "[t9] Fetched https://www.oracle.com/legal/privacy/privacy-choices.html\n",
      "[t9] Saved https://www.oracle.com/legal/privacy/privacy-choices.html\n",
      "[t9] Generating new URLs ... \n",
      "[t9] Generated 38 new URLs.\n",
      "[t9] Crawled www.oracle.com\n",
      "[t9] exiting ...\n",
      "[t1] Crawling en.wikipedia.org ... \n",
      "[t3] Crawling www.youtube.com ... \n",
      "[t1] Fetched https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "[t1] Saved https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "[t1] Generating new URLs ... \n",
      "[t1] Generated 1172 new URLs.\n",
      "[t1] Crawled en.wikipedia.org\n",
      "[t1] exiting ...\n",
      "[t3] Fetched https://www.youtube.com/oracle/\n",
      "[t3] Saved https://www.youtube.com/oracle/\n",
      "[t3] Generating new URLs ... \n",
      "[t3] Generated 12 new URLs.\n",
      "[t3] Crawled www.youtube.com\n",
      "[t3] exiting ...\n",
      "Scraped Count:  100\n",
      "Scraped URLs:\n",
      " {'https://en.wikipedia.org/wiki/Holger_H._Hoos', 'https://uk.wikipedia.org/wiki/Weka', 'https://en.wikipedia.org/w/api.php', 'https://en.wikipedia.org/wiki/File:Weka_(software)_logo.png', 'https://en.wikipedia.org/wiki/New_Zealand', 'https://en.wikipedia.org/wiki/Cluster_analysis', 'http://www.cs.waikato.ac.nz/~ml/publications/1999/99IHW-EF-LT-MH-GH-SJC-Tools-Java.pdf', 'https://id.wikipedia.org/wiki/Weka_(pembelajaran_mesin)', 'https://en.wikipedia.org/wiki/Software_categories', 'https://wikimediafoundation.org/', 'https://en.wikipedia.org/wiki/File:Weka-3.5.5.png', 'https://en.wikipedia.org/wiki/Category:Data_mining_and_machine_learning_software', 'https://en.wikipedia.org/wiki/Wolfram_Mathematica', 'https://en.wikipedia.org/wiki/Predictive_analytics', 'https://en.wikipedia.org/wiki/Endemism', 'https://www.oracle.com/legal/privacy/', 'https://en.wikipedia.org/wiki/Java_programming_language', 'https://www.oracle.com/internet-of-things/', 'https://en.wikipedia.org/wiki/Kevin_Leyton-Brown', 'https://lists.wikimedia.org/pipermail/mediawiki-api-announce/', 'https://en.wikipedia.org/wiki/Project_Jupyter', 'https://ca.wikipedia.org/wiki/Weka_(aprenentatge_autom%C3%A0tic)', 'https://en.wikipedia.org/wiki/Massive_Online_Analysis', 'https://de.wikipedia.org/wiki/Waikato_Environment_for_Knowledge_Analysis', 'http://www.pentaho.com/pentaho-acquires-weka-project', 'https://stats.wikimedia.org/', 'https://en.wikipedia.org/wiki/Maple_(software)', 'https://en.wikipedia.org/wiki/Linux', 'https://community.hitachivantara.com/s/article/machine-intelligence-made-easy', 'https://cs.wikipedia.org/wiki/Weka', 'https://www.oracle.com/corporate/', 'https://docs.oracle.com/developer/GUID-ABE2373F-287F-4C3A-BEBD-02F179F399FD.htm', 'http://www.acm.org/sigs/sigkdd/awards_service.php', 'https://en.wikipedia.org/wiki/IA-32', 'https://en.wikipedia.org/wiki/Julia_(programming_language)', 'https://docs.oracle.com/en/', 'https://commons.wikimedia.org/wiki/Category:Weka_(machine_learning)', 'https://fa.wikipedia.org/wiki/%D9%88%DA%A9%D8%A7_(%DB%8C%D8%A7%D8%AF%DA%AF%DB%8C%D8%B1%DB%8C_%D9%85%D8%A7%D8%B4%DB%8C%D9%86%DB%8C)', 'http://www.cs.waikato.ac.nz/~ml/weka', 'https://th.wikipedia.org/wiki/%E0%B9%80%E0%B8%A7%E0%B8%81%E0%B8%B2', 'https://en.wikipedia.org/wiki/Template_talk:Numerical_analysis_software', 'https://ja.wikipedia.org/wiki/Weka', 'https://www.cs.waikato.ac.nz/~ml/publications/1994/Holmes-ANZIIS-WEKA.pdf', 'https://www.oracle.com/corporate/careers/culture/who-we-are.html', 'https://en.wikipedia.org/wiki/Category:Free_science_software', 'https://en.wikipedia.org/wiki/Makefile', 'https://www.mediawiki.org/', 'https://en.wikipedia.org/wiki/Salome_(software)', 'http://weka.sourceforge.net/packageMetaData/', 'https://en.wikipedia.org/wiki/Wikipedia:Community_portal', 'https://en.wikipedia.org/wiki/Machine_learning', 'https://tr.wikipedia.org/wiki/Weka', 'https://svn.cms.waikato.ac.nz/svn/weka/', 'https://www.oracle.com/startup/', 'https://en.wikipedia.org/wiki/Wikipedia:Contact_us', 'https://en.wikipedia.org/wiki/Fortress_(programming_language)', 'https://en.wikipedia.org/wiki/Weka_(machine_learning)', 'https://pt.wikipedia.org/wiki/Weka', 'https://en.wikipedia.org/wiki/Scikit-learn', 'https://en.wikipedia.org/wiki/ScicosLab', 'https://en.wikipedia.org/wiki/Software_release_life_cycle', 'https://foundation.wikimedia.org/wiki/Cookie_statement', 'https://en.wikipedia.org/wiki/Python_(programming_language)', 'https://www.oracle.com/blockchain/', 'https://foundation.wikimedia.org/wiki/Privacy_policy', 'https://www.youtube.com/oracle/', 'https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html', 'https://www.oracle.com/artificial-intelligence/', 'https://foundation.wikimedia.org/wiki/Terms_of_Use', 'https://developer.oracle.com/', 'https://ru.wikipedia.org/wiki/Weka', 'https://si.wikipedia.org/wiki/%E0%B7%80%E0%B7%99%E0%B6%9A%E0%B7%8F_%E0%B6%B8%E0%B7%98%E0%B6%AF%E0%B7%94%E0%B6%9A%E0%B7%8F%E0%B6%82%E0%B6%9C%E0%B6%BA', 'https://www.wikidata.org/wiki/Q115494', 'https://www.oracle.com/corporate/careers/benefits/training.html', 'https://fr.wikipedia.org/wiki/Weka_(informatique)', 'https://es.wikipedia.org/wiki/Weka_(aprendizaje_autom%C3%A1tico)', 'https://zh.wikipedia.org/wiki/Weka', 'https://vi.wikipedia.org/wiki/Weka_(h%E1%BB%8Dc_m%C3%A1y)', 'https://www.oracle.com/corporate/careers/', 'https://it.wikipedia.org/wiki/Weka', 'https://www.oracle.com/corporate/careers/culture/diversity.html', 'https://www.csie.ntu.edu.tw/R_example.html', 'https://www.oracle.com/javadownload', 'https://www.oracle.com/', 'http://www.kdnuggets.com/news/2005/n13/2i.html', 'https://phabricator.wikimedia.org/maniphest/query/GebfyV4uCaLd/', 'https://www.wikimediafoundation.org/', 'https://en.wikipedia.org/api/', 'https://www.oracle.com/tel:18006330738', 'https://go.oracle.com/subscriptions', 'https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.459.8443', 'https://docs.oracle.com/middleware/jet210/jet/index.html', 'https://www.oracle.com/universal-menu/', 'https://dl.acm.org/citation.cfm?id=2487629', 'https://www.oracle.com/legal/privacy/privacy-choices.html', 'https://www.oracle.com/partnernetwork/', 'https://eu.wikipedia.org/wiki/Weka_(ikasketa_automatikoa)', 'https://sl.wikipedia.org/wiki/Weka_(strojno_u%C4%8Denje)', 'https://creativecommons.org/licenses/by-sa/3.0/', 'https://nl.wikipedia.org/wiki/Weka_(software)'}\n"
     ]
    }
   ],
   "source": [
    "# start the threads\n",
    "workers = []\n",
    "\n",
    "for i in range(THREADS):\n",
    "    workers.append(threading.Thread(target=crawler_thread_task, name = f't{i+1}', args=(URL_frontier,)))\n",
    "    \n",
    "for i in range(THREADS):\n",
    "    workers[i].start()\n",
    "\n",
    "for i in range(THREADS):\n",
    "    workers[i].join()\n",
    "\n",
    "print(\"[main] Scraped Count: \", len(CRAWLED_URLS))\n",
    "\n",
    "print(\"[main] Scraped URLs:\\n\", CRAWLED_URLS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0eec5928f4a012abf9585284ef5c28bee95330c0fbf5e596b340575e9a19f63d1",
   "display_name": "Python 3.8.5 64-bit ('ai-env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}